

## 1. ニューラルネットワークの学習体系

ニューラルネットワークの要素
　
1. 活性化関数
2. 損失関数
3. 勾配降下法
4. バッチサイズ

### 活性化関数

活性化関数は、パフォーマンスに直結する。  
いくつか種類があるが、代表的なのは以下。  
ディープラーニングでは、Relu関数が使われることが多い。

1. シグモイド関数
2. Relu関数(max関数とも呼ばれる)

これ以外の関数に以下のようなものがある。

* ステップ関数
* 恒等関数
* ソフトマックス関数

### 損失関数

出力値と正解値のずれの尺度が必要
代表的なものは、以下。

* 二乗誤差
* 交差エントロピー誤差

交差エントロピーは、誤差が大きければ、二乗誤差よりも収束が早い。

### 勾配降下法

全体の誤差が小さくなるようにNNの個々の重みを調整する。  
全体の誤差は、損失関数より求まる。

重みの更新アルゴリズム

* 確率的勾配降下法(SGD)
* AdaGrad
* Adam

Adamは様々なアルゴリズムの良い点を持っているので、よく使われるらしい。
一般には、確率的勾配降下法とAdamがよく使われるらしい。


### バッチサイズ

バッチサイズとは、重みの更新の実行間隔のこと

* バッチ学習  
    全学習データを使って、重みを一気に更新する。小さいデータセット向け
* オンライン学習  
    個々のデータごとに重みを更新する。大きいデータセット向け
* ミニバッチ学習  
    バッチ学習とオンライン学習の中間。ランダムなひとかたまりのデータごとに重みの更新を行う。

関連して大事な用語
* 1エポック  
    全額収容データを1回学習すること
* エポック数  
    エポックの繰り返し数


良い結果を得るためには、バッチサイズとエポック数の適切な設定が大事。

## 2. Chainerの基礎

Chainerの構成要素

1. Variable
2. Links
3. Chain
4. Functions
5. Optimizer


### Variable

NNの各層に対応  
Numpyの配列から生成する

```python
x = Variable(np.array([[1,2,3],[4,5,6]], dtype=np.float32))
```

### Links

* ニューラルネットワークの重みとバイアスを保持
* 関数のように振る舞い、Variableオブジェクトを変換
* 誤差の逆伝播により、各勾配が計算できる

```python
l = Links.Linear(3,2)
```

### Chain


* 複数のLinkオブジェクトを格納
* ニューラルネットワーク全体の管理
* 学習結果の保存/読み込み

```python
class MyChain(Chain):
    def __init__(self):
        super().__init__(
            l1=L.Linear(4,3),
            l2=L.Linear(3,2),
        )
    def __call__(self,x):
        h = self.l1(x)
        return self.l2(h)
```


### Functions

* Variableに演算を行うための関数群
* 活性化関数や損失関数などの様々な関数を含む

```python
y = chainer.functions.sigmonid(x)
```

### Optimizer

* 重みとバイアスの更新を行う
* SGDやAdamなどのアルゴリズムを指定する

```python
optimizer = chainer.optimizers.SGD()
```

## 3. Iris(あやめ)データセットを使った品種分類

### 概要

* 各花ごとに4つの測定値＋品種のデータ：全部で150組
* 測定値
    * Sepal length(萼の長さ)
    * Sepal width(萼の幅)
    * Petal lengh(花弁の長さ)
    * Petal width(花弁の幅)
* 品種
    * 0: Setosa
    * 1: Versicolor
    * 2: Versinica
* 4層からなるニューラルネットワーク
    * 入力層: 4つのニューロン(各測定値に対応)
    * 中間層: 2つ、それぞれ6つのニューロン
    * 出力層: 3つのニューロン(各品種に対応)
